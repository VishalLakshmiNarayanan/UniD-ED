{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00d440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    d_model: int = 1024\n",
    "    n_heads: int = 16\n",
    "    d_ff: int = 2816\n",
    "    vocab_size: int = 32000\n",
    "    num_encoder_layers: int = 6\n",
    "    num_decoder_layers: int = 3\n",
    "    rope_theta: float = 10000.0\n",
    "    dropout: float = 0.0\n",
    "    kv_heads: int = 4        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07337f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects input x into Q, K, and V for Multi-Head Attention.\n",
    "    Supports GQA (Grouped Query Attention).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        d_model = cfg.d_model\n",
    "\n",
    "        # head dimension based on full heads\n",
    "        self.head_dim = d_model // cfg.n_heads\n",
    "\n",
    "        # Query always has full heads\n",
    "        self.W_q = nn.Linear(d_model, cfg.n_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # K/V have fewer heads but same head_dim\n",
    "        kv_dim = cfg.kv_heads * self.head_dim\n",
    "        self.W_k = nn.Linear(d_model, kv_dim, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, kv_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq, _ = x.shape\n",
    "        cfg = self.cfg\n",
    "        hdim = self.head_dim\n",
    "\n",
    "        # projections\n",
    "        Q = self.W_q(x)                                # (B, S, n_heads * hdim)\n",
    "        K = self.W_k(x)                                # (B, S, kv_heads * hdim)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # reshape into heads\n",
    "        Q = Q.view(batch, seq, cfg.n_heads, hdim)      # (B, S, n_heads, hdim)\n",
    "        K = K.view(batch, seq, cfg.kv_heads, hdim)     # (B, S, kv_heads, hdim)\n",
    "        V = V.view(batch, seq, cfg.kv_heads, hdim)\n",
    "\n",
    "        return Q, K, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d5f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([2, 5, 16, 64])\n",
      "K shape: torch.Size([2, 5, 4, 64])\n",
      "V shape: torch.Size([2, 5, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "cfg = ModelConfig()\n",
    "proj = QKVProjection(cfg)\n",
    "\n",
    "x = torch.randn(2, 5, cfg.d_model)\n",
    "\n",
    "Q, K, V = proj(x)\n",
    "\n",
    "print(\"Q shape:\", Q.shape)\n",
    "print(\"K shape:\", K.shape)\n",
    "print(\"V shape:\", V.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
